\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}
\contentsline {section}{\numberline {1.1}About the task}{3}{section.1.1}
\contentsline {section}{\numberline {1.2}About the model}{3}{section.1.2}
\contentsline {section}{\numberline {1.3}Thesis goals}{4}{section.1.3}
\contentsline {section}{\numberline {1.4}Road map}{4}{section.1.4}
\contentsline {chapter}{\numberline {2}Computational model of semantics}{5}{chapter.2}
\contentsline {section}{\numberline {2.1}Motivation}{5}{section.2.1}
\contentsline {section}{\numberline {2.2}Similarity is proximity}{5}{section.2.2}
\contentsline {section}{\numberline {2.3}Distributional manifestation of meaning}{6}{section.2.3}
\contentsline {section}{\numberline {2.4}Context vectors}{7}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}A brief history of context vectors}{7}{subsection.2.4.1}
\contentsline {section}{\numberline {2.5}The co--occurrence matrix}{8}{section.2.5}
\contentsline {chapter}{\numberline {3}Word sense disambiguation}{11}{chapter.3}
\contentsline {section}{\numberline {3.1}Word sense(s)}{11}{section.3.1}
\contentsline {section}{\numberline {3.2}Applications of WSD-- why is it important?}{12}{section.3.2}
\contentsline {section}{\numberline {3.3}Classification of approaches to WSD}{13}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Unsupervised learning methods}{13}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Supervised methods}{14}{subsection.3.3.2}
\contentsline {section}{\numberline {3.4}Approaches that use VSM for WSD}{15}{section.3.4}
\contentsline {section}{\numberline {3.5}State--of--the--art}{15}{section.3.5}
\contentsline {chapter}{\numberline {4}Word space implementations}{17}{chapter.4}
\contentsline {section}{\numberline {4.1}High dimensionality and data sparseness}{17}{section.4.1}
\contentsline {section}{\numberline {4.2}Dimensionality reduction}{17}{section.4.2}
\contentsline {section}{\numberline {4.3}Latent Semantic Analysis}{18}{section.4.3}
\contentsline {section}{\numberline {4.4}Hyperspace Analogue to Language}{20}{section.4.4}
\contentsline {section}{\numberline {4.5}Random Indexing}{21}{section.4.5}
\contentsline {chapter}{\numberline {5}The theory behind experiments}{23}{chapter.5}
\contentsline {section}{\numberline {5.1}Linguistic preprocessing}{23}{section.5.1}
\contentsline {section}{\numberline {5.2}Statistic preprocessing}{24}{section.5.2}
\contentsline {section}{\numberline {5.3}Types and Tokens}{24}{section.5.3}
\contentsline {section}{\numberline {5.4}Building the matrix}{26}{section.5.4}
\contentsline {subsection}{\numberline {5.4.1}Normalizing the frequency counts}{26}{subsection.5.4.1}
\contentsline {subsection}{\numberline {5.4.2}TF--IDF}{26}{subsection.5.4.2}
\contentsline {subsection}{\numberline {5.4.3}PMI}{27}{subsection.5.4.3}
\contentsline {section}{\numberline {5.5}Calculating similarity}{28}{section.5.5}
\contentsline {chapter}{\numberline {6}Experiments}{30}{chapter.6}
\contentsline {section}{\numberline {6.1}The resource}{30}{section.6.1}
\contentsline {subsection}{\numberline {6.1.1}Polysemous words}{31}{subsection.6.1.1}
\contentsline {subsection}{\numberline {6.1.2}Why PDT1 instead of PDT2?}{31}{subsection.6.1.2}
\contentsline {section}{\numberline {6.2}Preprocessing}{32}{section.6.2}
\contentsline {section}{\numberline {6.3}Train and test sets}{34}{section.6.3}
\contentsline {section}{\numberline {6.4}Document size}{34}{section.6.4}
\contentsline {section}{\numberline {6.5}Normalization of the frequency counts and dimensionality reduction}{36}{section.6.5}
\contentsline {section}{\numberline {6.6}Evaluation context size}{37}{section.6.6}
\contentsline {section}{\numberline {6.7}Evaluation metrics and baselines}{38}{section.6.7}
\contentsline {section}{\numberline {6.8}Tuning}{39}{section.6.8}
\contentsline {subsection}{\numberline {6.8.1}Tuning preprocessing parameters}{39}{subsection.6.8.1}
\contentsline {subsection}{\numberline {6.8.2}Tuning evaluation context size}{40}{subsection.6.8.2}
\contentsline {subsection}{\numberline {6.8.3}Evaluation on polysemous words of different level }{40}{subsection.6.8.3}
\contentsline {section}{\numberline {6.9}TF--IDF experiments}{41}{section.6.9}
\contentsline {subsection}{\numberline {6.9.1}Tuning preprocessing parameters}{41}{subsection.6.9.1}
\contentsline {subsection}{\numberline {6.9.2}Tuning evaluation context size}{44}{subsection.6.9.2}
\contentsline {subsection}{\numberline {6.9.3} Evaluation on polysemous words of different level}{45}{subsection.6.9.3}
\contentsline {section}{\numberline {6.10}PMI experiments}{46}{section.6.10}
\contentsline {subsection}{\numberline {6.10.1}Tuning preprocessing parameters}{46}{subsection.6.10.1}
\contentsline {chapter}{\numberline {7}Implementation}{47}{chapter.7}
\contentsline {section}{\numberline {7.1}Data flow}{47}{section.7.1}
\contentsline {chapter}{\numberline {8}User manual}{49}{chapter.8}
\contentsline {section}{\numberline {8.1}Input parameters}{49}{section.8.1}
\contentsline {section}{\numberline {8.2}Logs}{49}{section.8.2}
\contentsline {chapter}{Summary}{50}{chapter*.2}
\contentsline {chapter}{List off Tables}{55}{chapter*.4}
\contentsline {chapter}{List of Abbreviations}{56}{chapter*.5}
\contentsline {chapter}{Appendices}{57}{chapter*.6}
\contentsline {chapter}{\numberline {9}List of filter words for Czech language}{58}{chapter.9}
